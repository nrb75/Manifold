{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import random\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import pyfpgrowth\n",
    "from format_rules import format_rules\n",
    "#from server_association import server_association\n",
    "from Server_Assign import server_association\n",
    "#from apyori import apriori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#df=pd.read_csv('/home/natalie/Documents/Manifold/df_test.csv')\n",
    "client = boto3.client('s3')\n",
    "obj = client.get_object(Bucket='manifolddata', Key='week1.csv')\n",
    "df = pd.read_csv(BytesIO(obj['Body'].read()), low_memory=False)\n",
    "\n",
    "\n",
    "df=df.iloc[:,[0,1,3,4,5,6,7,8]]\n",
    "df.columns=['Date', 'Duration', 'Src_IP', 'Src_pt', 'Dst_IP', 'Dst_pt','Packets', 'Bytes']\n",
    "#add an date column that is rounded to nearest hour, so we can use this as a timestep to see how frequently IP pairs occur in each timestep\n",
    "df['Date']=pd.to_datetime(df['Date'], format=\"%Y-%m-%d %H:%M:%S.%f\", errors = 'coerce')\n",
    "df['date_hr']=pd.Series(df['Date']).dt.round(\"H\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pair column, which is a touple of the src and dst IP, sorted. \n",
    "#It does not matter which call came first, we simply want to know which pair occurs most frequently.\n",
    "\n",
    "df['pairs']=list(zip(df.Src_IP, df.Dst_IP))\n",
    "df['pairs']=df['pairs'].apply(sorted)\n",
    "df['pairs2']=tuple(df['pairs'])\n",
    "#create a normalized latency column = duration/packets\n",
    "df['norm_latency']=df['Duration']/df['Packets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_si_to_number(x):\n",
    "    total_stars = 0\n",
    "    if 'k' in x:\n",
    "        if len(x) > 1:\n",
    "            total_stars = float(x.replace('k', '')) * 1000 # convert k to a thousand\n",
    "    elif 'M' in x:\n",
    "        if len(x) > 1:\n",
    "            total_stars = float(x.replace('M', '')) * 1000000 # convert M to a million\n",
    "    elif 'B' in x:\n",
    "        total_stars = float(x.replace('B', '')) * 1000000000 # convert B to a Billion\n",
    "    else:\n",
    "        total_stars = int(x) # Less than 1000\n",
    "    return int(total_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Bytes=df.Bytes.astype('str')\n",
    "test_list=df.Bytes\n",
    "\n",
    "[i for i, s in enumerate(test_list) if 'M' in s]#show where the M errors are happening\n",
    "\n",
    "test_list= [convert_si_to_number(x) for x in test_list]\n",
    "df.Bytes=test_list #bring it back into the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add hour column, where it is a value 1-24, and repeats each day\n",
    "df['hour']=None\n",
    "df['hour']=pd.DatetimeIndex(df['Date']).hour\n",
    "\n",
    "#lambda x: x[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many transactions do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Frequent Pattern (FP) - Growth algorithm\n",
    "\n",
    "https://fp-growth.readthedocs.io/en/latest/usage.html\n",
    "\n",
    "This algorithm\n",
    "1. Counts occurence of items in dataset in 1st pass\n",
    "2. Build FP-tree by inserting instances, and adds count to each instance based on the number of times it appears in the dataset. Those infrequent instances are dropped from the tree.\n",
    "\n",
    "This allows the frequentest set to be generated organically, instead of creating a list of each itemset and checking if it does/does not pass minimum threshold (like Apriori algorithm)\n",
    "\n",
    "#### Vocabulary\n",
    " - itemset = all items in 1 transaction\n",
    "A \"pattern\" is a conjunction of items, or the unique itemset.\n",
    "A \"rule\" X --> Y means if you buy X you are likely to buy Y, or in this case if X IP address is used Y IP address is likely also\n",
    "\n",
    "\n",
    "#### Evaluation metrics include:\n",
    "\n",
    "1. Support = how frequently it occurs. The number of transactions of that unique itemset / all transactions. In this case: number of times IP pair occurs/all requests\n",
    "\n",
    "2. Confidence = how often rule is likely to be true. frequency of X and Y occuring/ frequency of X occuring in entire dataset. Conditional probability of Y given X. P(Ey|Ex)\n",
    "\n",
    "3. Lift = How likely is item Y given item X occurs, controlling for how frequent Y occurs in the entire dataset. For rule X-->Y, lift = P(Y|X)/P(Y).\n",
    "    Lift = 1 means X and Y are independent\n",
    "    Lift >1 = X and Y are positively correlated\n",
    "    Lift <1 = X and Y are negatively correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement Frequentest paterns algorithm\n",
    "\n",
    "Before we run this algorithm, we should have some idea of the minimum threshold we want to set. This is the \"support\" for each IP pair. Support = % of time this pair occurs in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do this for each hour in the dataset, we think that frequency in pairs might change with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_groups=[]\n",
    "\n",
    "for i in range(0,25):\n",
    "    data=df[df['hour']==i]\n",
    "    data_groups.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make individual 'pairs count' frequency tables so we can get a good value for our minimum support and keep it consitent.\n",
    "#We are selecting the 80th percentile as our min threshold, so each group will ahve a different 80th value.\n",
    "\n",
    "pairs_count=(df.groupby('pairs2').agg({'Date':'count', 'norm_latency': 'mean', 'Duration': 'sum', 'Packets':'sum'}).reset_index())\n",
    "pairs_count.columns=['pairs','frequency', 'avg_norm_latency', 'total_duration', 'total_packets']\n",
    "pairs_count['norm_latency']=(pairs_count['total_duration']/pairs_count['total_packets'].sum())*100 #sum of all duration time divided by \n",
    "np.percentile(pairs_count['frequency'], [80])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_list=[]\n",
    "per80_list=[]\n",
    "data_l_list=[]\n",
    "patterns_list=[]\n",
    "rules_list=[]\n",
    "confidence=0.7 #this means the rule is likely to be true 20% of the time, it is a high threshold, used for testing\n",
    "\n",
    "\n",
    "for i in data_groups:    \n",
    "    data_l=list(i['pairs'])\n",
    "    pairs_count=(i.groupby('pairs2').agg({'Date':'count', 'norm_latency': 'mean', 'Duration': 'sum', 'Packets':'sum'}).reset_index())\n",
    "    pairs_count.columns=['pairs','frequency', 'avg_norm_latency', 'total_duration', 'total_packets']\n",
    "    pairs_count['norm_latency']=(pairs_count['total_duration']/pairs_count['total_packets'].sum())*100 #sum of all duration time divided by sum of all packets transfered for that pair\n",
    "    pairs_list.append(pairs_count)\n",
    "    per_80=(pairs_count['frequency'].quantile(.8))\n",
    "    patterns = pyfpgrowth.find_frequent_patterns(data_l, per_80) \n",
    "    patterns_list.append(patterns)\n",
    "    rules = pyfpgrowth.generate_association_rules(patterns, confidence)\n",
    "    rules_list.append(rules)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the rules, bring back in the other info on latency rank\n",
    "rules_list=rules_list[:-1]#last item was empty, remove it\n",
    "formated_rules=[]\n",
    "apps_server=20\n",
    "\n",
    "for i in rules_list:\n",
    "    formatrule=format_rules(i, df, apps_server)\n",
    "    formated_rules.append(formatrule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Servers to each IP address in the rules dataframe\n",
    "\n",
    "Start with the highest rank IP pair, assign matching servers and then move on to the next highest rank pair until that server is full. This is a 'dumb' approach, but  it clearly assigns things based on priority and gets a proof of concept. Because IP addresses repeat throughout the rules dataset, we will remove repeated IPs leaving the highest ranked IP.\n",
    "\n",
    "For this exercise we assume a server can hold 20 apps, or 10 pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(formated_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign IPs to the servers at each hour\n",
    "server_assign_list=[]\n",
    "total_latency_list=[]\n",
    "total_latency_model_list=[]\n",
    "avg_latency_list=[]\n",
    "avg_latency_model_list=[]\n",
    "\n",
    "\n",
    "for i, j in zip(formated_rules, data_groups) :\n",
    "    server_df, server_assignments, total_latency, total_latency_model, avg_latency, avg_latency_model = server_association(i, j, apps_server) #this function loaded fr\n",
    "    server_assign_list.append(server_assignments)\n",
    "    total_latency_list.append(total_latency)\n",
    "    total_latency_model_list.append(total_latency_model)\n",
    "    avg_latency_list.append(avg_latency)\n",
    "    avg_latency_model_list.append(avg_latency_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original times\n",
    "\n",
    "df_hour=df.groupby('hour')['Duration'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring together all the durations for the actual data and the model\n",
    "hours=range(0,24)\n",
    "model_output=pd.DataFrame({'hours':hours,'total_latency_list': total_latency_list, 'total_latency_model_list': total_latency_model_list, 'avg_latency_list': avg_latency_list, 'avg_latency_model_list': avg_latency_model_list})\n",
    "model_output.columns=['hours', 'total_latency', 'total_latency_model', 'avg_latency', 'avg_latency_model']\n",
    "model_output['avg_latency_per_reduction']=((model_output['avg_latency']-model_output['avg_latency_model'])/model_output['avg_latency'])*100\n",
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time=['12am', '1am', '2am', '3am', '4am', '5am', '6am', '7am', '8am', '9am', '10am', '11am', '12pm', '1pm', '2pm', '3pm', '4ppm', '5pm', '6pm', '7pm', '8pm', '9pm', '10pm', '11pm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did this model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output['total_latency'].sum() #original total latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output['total_latency_model'].sum()#predicted latency, does not match the other output.... what is wrong..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall, this is a % improvement in total latency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model_output['total_latency'].sum()-model_output['total_latency_model'].sum())/model_output['total_latency'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about average transaction time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output['avg_latency'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output['avg_latency_model'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each hour the avg transaction time is averaged for a overall avg\n",
    "\n",
    "model_output['avg_latency_per_reduction'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(model_output['hours'], model_output['total_latency'])\n",
    "ax.plot(model_output['hours'], model_output['total_latency_model'])\n",
    "\n",
    "\n",
    "## Rotate date labels automatically\n",
    "fig.autofmt_xdate()\n",
    "plt.ylabel('Total latency (s)')\n",
    "plt.xlabel('hour of day')\n",
    "plt.title('Total latency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(7,5))\n",
    "ax.plot(model_output['hours'], model_output['avg_latency'], color='steelblue', linewidth=2.0)\n",
    "ax.plot(model_output['hours'], model_output['avg_latency_model'], color='firebrick', linewidth=4.0)\n",
    "\n",
    "\n",
    "## Rotate date labels automatically\n",
    "fig.autofmt_xdate()\n",
    "plt.ylabel('Latency (s)')\n",
    "plt.xlabel('hour of day')\n",
    "plt.title('Average Transaction Latency')\n",
    "fig.savefig('Average_transaction_latency_hours_day1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same analysis but subset the data for training and testing\n",
    "\n",
    "use the first 5 days for training, and the last 2 days for testing. Even though this covers weekends, which are in the training dataset, because of the relatively small amount of transactions on the weekend, it is not likely to skew the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=df[df['Date']<'2017-08-08'] #first 6 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=df[df['Date']>'2017-08-08'] #last day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "\n",
    "data_groups_t=[]\n",
    "\n",
    "for i in range(0,25):\n",
    "    data=train_df[train_df['hour']==i]\n",
    "    data_groups_t.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_groups=[]\n",
    "for i in data_groups_t:\n",
    "    len_groups.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make individual 'pairs count' frequency tables so we can get a good value for our minimum support and keep it consitent.\n",
    "#We are selecting the 80th percentile as our min threshold, so each group will ahve a different 80th value.\n",
    "\n",
    "pairs_count_t=(train_df.groupby('pairs2').agg({'Date':'count', 'norm_latency': 'mean', 'Duration': 'sum', 'Packets':'sum'}).reset_index())\n",
    "pairs_count_t.columns=['pairs','frequency', 'avg_norm_latency', 'total_duration', 'total_packets']\n",
    "pairs_count_t['norm_latency']=(pairs_count_t['total_duration']/pairs_count_t['total_packets'].sum())*100 #sum of all duration time divided by \n",
    "np.percentile(pairs_count_t['frequency'], [80])[0]\n",
    "\n",
    "pairs_list_t=[]\n",
    "per80_list_t=[]\n",
    "data_l_list_t=[]\n",
    "patterns_list_t=[]\n",
    "rules_list_t=[]\n",
    "confidence=0.7 #this means the rule is likely to be true 70% of the time\n",
    "\n",
    "\n",
    "for i in data_groups_t:    \n",
    "    data_l_t=list(i['pairs'])#the IP pairs are now a list of lists\n",
    "    pairs_count_t=(i.groupby('pairs2').agg({'Date':'count', 'norm_latency': 'mean', 'Duration': 'sum', 'Packets':'sum'}).reset_index())\n",
    "    pairs_count_t.columns=['pairs','frequency', 'avg_norm_latency', 'total_duration', 'total_packets']\n",
    "    pairs_count_t['norm_latency']=(pairs_count_t['total_duration']/pairs_count_t['total_packets'].sum())*100 #sum of all duration time divided by sum of all packets transfered for that pair\n",
    "    pairs_list_t.append(pairs_count_t)\n",
    "    per_80=(pairs_count_t['frequency'].quantile(.8))\n",
    "    patterns = pyfpgrowth.find_frequent_patterns(data_l_t, per_80) \n",
    "    patterns_list_t.append(patterns)\n",
    "    rules = pyfpgrowth.generate_association_rules(patterns, confidence)\n",
    "    rules_list_t.append(rules)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the rules, bring back in the other info on latency rank, based on the training data\n",
    "rules_list_t=rules_list_t[:-1]#last item was empty, remove it\n",
    "formated_rules_train=[]\n",
    "apps_server=20\n",
    "\n",
    "for i in rules_list_t:\n",
    "    formatrule=format_rules(i, train_df, apps_server)\n",
    "    formated_rules_train.append(formatrule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_groups_test=[]\n",
    "\n",
    "for i in range(0,25):\n",
    "    data=test_df[test_df['hour']==i]\n",
    "    data_groups_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign IPs to the TEST servers at each hour, \n",
    "#NOW WE ASSIGN SERVERS TO THE TETING DATA BASED ON THE TRAINING DATA USED ABOVE\n",
    "\n",
    "server_assign_list_test=[]\n",
    "total_latency_list_test=[]\n",
    "total_latency_model_list_test=[]\n",
    "avg_latency_list_test=[]\n",
    "avg_latency_model_list_test=[]\n",
    "df_server_list=[]\n",
    "\n",
    "\n",
    "for i, j in zip(formated_rules_train, data_groups_test) :\n",
    "    df_servers, server_assignments, total_latency, total_latency_model, avg_latency, avg_latency_model = server_association(i, j, apps_server) #this function loaded fr\n",
    "    df_server_list.append(df_servers)\n",
    "    server_assign_list_test.append(server_assignments)\n",
    "    total_latency_list_test.append(total_latency)\n",
    "    total_latency_model_list_test.append(total_latency_model)\n",
    "    avg_latency_list_test.append(avg_latency)\n",
    "    avg_latency_model_list_test.append(avg_latency_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring together all the durations for the actual data and the model\n",
    "hours=range(0,24)\n",
    "model_output_test=pd.DataFrame({'hours':hours,'total_latency_list': total_latency_list_test, 'total_latency_model_list': total_latency_model_list_test, 'avg_latency_list': avg_latency_list_test, 'avg_latency_model_list': avg_latency_model_list_test})\n",
    "model_output_test.columns=['hours', 'total_latency', 'total_latency_model', 'avg_latency', 'avg_latency_model']\n",
    "model_output_test['avg_latency_per_reduction']=((model_output_test['avg_latency']-model_output_test['avg_latency_model'])/model_output_test['avg_latency'])*100\n",
    "model_output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib as matplotlib\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "fig, ax = plt.subplots(1,1, figsize=(7,5))\n",
    "ax.plot(model_output_test['hours'], model_output_test['avg_latency'], color='steelblue', linewidth=2.0)\n",
    "ax.plot(model_output_test['hours'], model_output_test['avg_latency_model'], color='firebrick', linewidth=4.0)\n",
    "\n",
    "\n",
    "## Rotate date labels automatically\n",
    "fig.autofmt_xdate()\n",
    "plt.ylabel('Latency (s)')\n",
    "plt.xlabel('hour of day')\n",
    "plt.title('Average Transaction Latency Test Data')\n",
    "fig.savefig('Average_transaction_latency_hours_day_Test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are doing a good job getting rid of the peak, this is because this is the only timeblock that had a lot of IPs pass the min threshold of the 80th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_test['total_latency'].sum()#total latency, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_test['total_latency_model'].sum()#predicted latency, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed how much better, %:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model_output_test['total_latency'].sum()-model_output_test['total_latency_model'].sum())/model_output_test['total_latency'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average transactional time before and after the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_test['avg_latency'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_test['avg_latency_model'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model_output_test['avg_latency'].mean()-model_output_test['avg_latency_model'].mean())/model_output_test['avg_latency'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same hourly step through, but with a min threshold of 50th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_list_t=[]\n",
    "per50_list_t=[]\n",
    "data_l_list_t=[]\n",
    "patterns_list_t=[]\n",
    "rules_list_t=[]\n",
    "confidence=0.7 #this means the rule is likely to be true 70% of the time\n",
    "\n",
    "\n",
    "for i in data_groups_t:    \n",
    "    data_l_t=list(i['pairs'])#the IP pairs are now a list of lists\n",
    "    pairs_count_t=(i.groupby('pairs2').agg({'Date':'count', 'norm_latency': 'mean', 'Duration': 'sum', 'Packets':'sum'}).reset_index())\n",
    "    pairs_count_t.columns=['pairs','frequency', 'avg_norm_latency', 'total_duration', 'total_packets']\n",
    "    pairs_count_t['norm_latency']=(pairs_count_t['total_duration']/pairs_count_t['total_packets'].sum())*100 #sum of all duration time divided by sum of all packets transfered for that pair\n",
    "    pairs_list_t.append(pairs_count_t)\n",
    "    per_50=(pairs_count_t['frequency'].quantile(.2))\n",
    "    patterns = pyfpgrowth.find_frequent_patterns(data_l_t, per_50) \n",
    "    patterns_list_t.append(patterns)\n",
    "    rules = pyfpgrowth.generate_association_rules(patterns, confidence)\n",
    "    rules_list_t.append(rules)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the rules, bring back in the other info on latency rank, based on the training data\n",
    "rules_list_t=rules_list_t[:-1]#last item was empty, remove it\n",
    "formated_rules_train=[]\n",
    "apps_server=20\n",
    "\n",
    "for i in rules_list_t:\n",
    "    formatrule=format_rules(i, train_df, apps_server)\n",
    "    formated_rules_train.append(formatrule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_groups_test=[]\n",
    "\n",
    "for i in range(0,25):\n",
    "    data=test_df[test_df['hour']==i]\n",
    "    data_groups_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign IPs to the TEST servers at each hour, \n",
    "#NOW WE ASSIGN SERVERS TO THE TETING DATA BASED ON THE TRAINING DATA USED ABOVE\n",
    "\n",
    "server_assign_list_test=[]\n",
    "total_latency_list_test=[]\n",
    "total_latency_model_list_test=[]\n",
    "avg_latency_list_test=[]\n",
    "avg_latency_model_list_test=[]\n",
    "df_server_list=[]\n",
    "\n",
    "\n",
    "for i, j in zip(formated_rules_train, data_groups_test) :\n",
    "    df_servers, server_assignments, total_latency, total_latency_model, avg_latency, avg_latency_model = server_association(i, j, apps_server) #this function loaded fr\n",
    "    df_server_list.append(df_servers)\n",
    "    server_assign_list_test.append(server_assignments)\n",
    "    total_latency_list_test.append(total_latency)\n",
    "    total_latency_model_list_test.append(total_latency_model)\n",
    "    avg_latency_list_test.append(avg_latency)\n",
    "    avg_latency_model_list_test.append(avg_latency_model)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring together all the durations for the actual data and the model\n",
    "hours=range(0,24)\n",
    "model_output_test50=pd.DataFrame({'hours':hours,'total_latency_list': total_latency_list_test, 'total_latency_model_list': total_latency_model_list_test, 'avg_latency_list': avg_latency_list_test, 'avg_latency_model_list': avg_latency_model_list_test})\n",
    "model_output_test50.columns=['hours', 'total_latency', 'total_latency_model', 'avg_latency', 'avg_latency_model']\n",
    "model_output_test50['avg_latency_per_reduction']=((model_output_test50['avg_latency']-model_output_test50['avg_latency_model'])/model_output_test50['avg_latency'])*100\n",
    "model_output_test50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "fig, ax = plt.subplots(1,1, figsize=(7,5))\n",
    "ax.plot(model_output_test50['hours'], model_output_test50['avg_latency'], color='steelblue', linewidth=2.0)\n",
    "ax.plot(model_output_test50['hours'], model_output_test50['avg_latency_model'], color='firebrick', linewidth=4.0)\n",
    "\n",
    "\n",
    "## Rotate date labels automatically\n",
    "fig.autofmt_xdate()\n",
    "plt.ylabel('Latency (s)')\n",
    "plt.xlabel('hour of day')\n",
    "plt.title('Average Transaction Latency Test Data')\n",
    "fig.savefig('Average_transaction_latency_hours_day_Test50.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only reducing the min threshold from 80th percentile to 50th did not produce a big difference.\n",
    "#Lowering it to to the 20th percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this compare to randomly assigning servers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out how many IPs were assigned based on our model, then randomly assign servers to the same amount of IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server_test = pd.concat(df_server_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthts=[]\n",
    "for i in df_server_list:\n",
    "    lengthts.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lengthts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_server_test) #should match originial test data length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to assign random servers to the IP addressed, however b/c we did not assign a server to all the IP addresses in our model run, we will only assign servers to the same proportion of the data.\n",
    "\n",
    "#how many servers did we assign in the Src_server column?\n",
    "num_src_assign=test_df['Src_Server'].count()\n",
    "num_dst_assign=test_df['Dst_Server'].count()\n",
    "\n",
    "xx_src=np.random.choice(len(test_df), num_src_assign) #which rows are randomly assigned src servers\n",
    "xx_dst=np.random.choice(len(test_df), num_dst_assign) #which rows are randomly assigned dst servers\n",
    "num_servers=server_rules['serverid'].nunique()\n",
    "test_df['rnd_src_server']=None\n",
    "test_df['rnd_dst_server']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_src_servers=[]\n",
    "for x in range(len(xx_src)):\n",
    "  rnd_src_servers.append(random.randint(0,num_servers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_dst_servers=[]\n",
    "for x in range(len(xx_dst)):\n",
    "  rnd_dst_servers.append(random.randint(0,num_servers)) #create random numbers for dst serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['rnd_src_server'][xx_src]=rnd_src_servers\n",
    "test_df['rnd_dst_server'][xx_dst]=rnd_dst_servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df[df_servers.rnd_src_server==df_servers.rnd_dst_server])/len(df_servers) #we only get 0.9% matching pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_servers['duration_rnd']=df_servers['Duration']\n",
    "df_servers['duration_rnd'][df_servers['rnd_src_server']==df_servers['rnd_dst_server']]=0 #if random servers match, set latency time=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
